{% extends 'base.html' %}

{% block header %}
<h1>{% block title %}New Run - {{ title }} - Advanced Settings{% endblock %}</h1>
{% endblock %}

{% block content %}
<form method=post>
    <h2>Information</h2>
    <p><i>
        This section allows the user to control fine details concerning training the tabular CGAN.
        If left blank, defaults will be used. Defaults will be noted throughout.
    </i></p>
    <hr>

    <h2>Learning Rates</h2>
    <p><i>
        Specify the learning rates for the generator, discriminator, and evaluator networks.
        The default for the generator is {{ default_params.netG_lr }} the default for the discriminator is {{ default_params.netD_lr }},
        and the default for the evaluator is {{ default_params.netE_lr }}.
    </i></p>
    <b>Generator: </b><input name="netG_lr" id="netG_lr" value="{{ request.form['netG_lr'] }}"><br>
    <b>Discriminator: </b><input name="netD_lr" id="netD_lr" value="{{ request.form['netD_lr'] }}"><br>
    <b>Evaluator: </b><input name="netE_lr" id="netE_lr" value="{{ request.form['netE_lr'] }}"><br>
    <hr>

    <h2>Adam Optimizer Beta1 and Beta2</h2>
    <p><i>
        Specify beta1 and beta2 for the generator discriminator, and evaluator networks.
        The default for all networks is beta1 = {{ default_params.netG_beta1 }} and beta2 = {{ default_params.netG_beta2 }}.
    </i></p>
    <b>Generator Beta1: </b><input name="netG_beta1" id="netG_beta1" type="number" min="0" max="1" step="0.1" value="{{ request.form['netG_beta1'] }}"><br>
    <b>Generator Beta2: </b><input name="netG_beta2" id="netG_beta2" type="number" min="0" max="1" step="0.001" value="{{ request.form['netG_beta2'] }}"><br>
    <b>Discriminator Beta1: </b><input name="netD_beta1" id="netD_beta1" type="number" min="0" max="1" step="0.1" value="{{ request.form['netD_beta1'] }}"><br>
    <b>Discriminator Beta2: </b><input name="netD_beta2" id="netD_beta2" type="number" min="0" max="1" step="0.001" value="{{ request.form['netD_beta2'] }}"><br>
    <b>Evaluator Beta1: </b><input name="netE_beta1" id="netE_beta1" type="number" min="0" max="1" step="0.1" value="{{ request.form['netE_beta1'] }}"><br>
    <b>Evaluator Beta2: </b><input name="netE_beta2" id="netE_beta2" type="number" min="0" max="1" step="0.001" value="{{ request.form['netE_beta2'] }}"><br>
    <hr>

    <h2>Weight Decay</h2>
    <p><i>
        Specify the amount of weight decay to apply to each network.
        The default amount of weight decay for all networks is {{ default_params.netG_wd }}.
    </i></p>
    <b>Generator: </b><input name="netG_wd" id="netG_wd" type="number" min="0" step="0.01" value="{{ request.form['netG_wd'] }}"><br>
    <b>Discriminator: </b><input name="netD_wd" id="netD_wd" type="number" min="0" step="0.01" value="{{ request.form['netD_wd'] }}"><br>
    <b>Evaluator: </b><input name="netE_wd" id="netE_wd" type="number" min="0" step="0.01" value="{{ request.form['netE_wd'] }}"><br>
    <hr>

    <h2>Label Noise</h2>
    <p><i>
        With probability label_noise, the labels for the discriminator's training (real or fake) are randomly flipped.
        For example, when training the discriminator, we traditionally feed it an entire batch of real data (labeled real),
        followed by an entire batch of fake data (labeled fake), and then update it based on how well it performed.
        In this case, when we feed it an entire batch of real data, we flip a percent of the labels to fake,
        so that we are feeding the discriminator some incorrect information (we do this for the fake data as well, labeling them as real).
        This results in inhibiting the discriminator's training process and tries to prevent the discriminator from learning simple tricks to
        detect whether an image is real or fake. The default for this value is {{ default_params.label_noise }}.
    </i></p>
    <b>Label Noise: </b><input name="label_noise" id="label_noise" type="number" min="0" max="1" step="0.01" value="{{ request.form['label_noise'] }}"><br>
    <hr>

    <h2>Linearly Anneal Label Noise</h2>
    <p><i>
        Linearly annealing label noise will decrease the label noise to 0 linearly as the network trains.
        The default for this value is {{ default_params.label_noise_linear_anneal }}.
    </i></p>
    <input type="radio" name="label_noise_linear_anneal" id="label_noise_linear_anneal" value="True">Yes, linearly anneal label noise</input><br>
    <input type="radio" name="label_noise_linear_anneal" id="label_noise_linear_anneal" value="False">No, do not linearly anneal label noise</input><br>
    <hr>

    <h2>Discriminator Noise</h2>
    <p><i>
        This value is the standard deviation of normally distributed noise (with mean zero) added to the discriminator's input data each time it evaluates
        real or fake data. This is a general trick employed in deep learning to improve robustness of models to noise, but also further prevents the
        discriminator from getting too good at discriminating real data from fake data based on trivial inconsistencies.
        The default for this value is {{ default_params.discrim_noise }}.
    </i></p>
    <b>Label Noise: </b><input name="discrim_noise" id="discrim_noise" type="number" min="0" step="0.01" value="{{ request.form['discrim_noise'] }}"><br>
    <hr>

    <h2>Linearly Anneal Discriminator Noise</h2>
    <p><i>
        Linearly annealing discriminator noise will decrease the discriminator noise to 0 linearly as the network trains.
        The default for this value is {{ default_params.discrim_noise_linear_anneal }}.
    </i></p>
    <input type="radio" name="discrim_noise_linear_anneal" id="discrim_noise_linear_anneal" value="True">Yes, linearly anneal disriminator noise</input><br>
    <input type="radio" name="discrim_noise_linear_anneal" id="discrim_noise_linear_anneal" value="False">No, do not linearly anneal disriminator noise</input><br>
    <hr>

    <h2>Size of Noise Vector</h2>
    <p><i>
        The noise vector is a vector of specified length of random values generated from the standard normal distribution that is fed to the generator in order to produce
        outputs. While the length is an important value, as long as the correct general range is used, the results shouldn't be overly sensitive to the specific value chosen.
        The default for this value is {{ default_params.nz }}.
    </i></p>
    <b>Noise Vector Length: </b><input name="nz" id="nz" type="number" min="0" step="1" value="{{ request.form['nz'] }}"><br>
    <hr>

    <h2>Training Schedule</h2>
    <p><i>
        A recurring problem with training GANs is that the discriminator dominates the generator and thus the generator is unable to receive a useful training signal.
        This value represents the number of batches to train the generator with for each set of batches the discriminator is trained with. Note that a value of 1 means
        that the discriminator is trained on twice as much data, since the discriminator receives a full batch each of real and fake data (for each set of data generated by
        the generator). The default for this value is {{ default_params.sched_netG }}.
    </i></p>
    <b>Training Schedule: </b><input name="sched_netG" id="sched_netG" type="number" min="1" step="1" value="{{ request.form['sched_netG'] }}"><br>
    <hr>

    <h2>Maximum Feature Map Size</h2>
    <p><i>
        Specify the maximum number of feature maps to use per layer in each network (essentially the width of the network).
        The number of feature maps per layer will scale similarly to high-performing architectures like ResNet.
        The default for the generator is {{ default_params.netG_nf }} feature maps and the default for the discriminator is {{ default_params.netD_nf }} feature maps.
    </i></p>
    <b>Generator Max Feature Maps: </b><input name="netG_nf" id="netG_nf" type="number" min="0" step="1" value="{{ request.form['netG_nf'] }}"><br>
    <b>Discriminator Max Feature Maps: </b><input name="netD_nf" id="netD_nf" type="number" min="0" step="1" value="{{ request.form['netD_nf'] }}"><br>
    <hr>

    <h2>Generated Data Set Size</h2>
    <p><i>
    Specify the size of the generated data set to use for training the evaluator. The default for this value is {{ default_params.fake_data_set_size }}.
    </i></p>
    <b>Generated Data Set Size: </b><input name="fake_data_set_size" id="fake_data_set_size" type="number" min="0" max="1" step="0.01" value="{{ request.form['fake_data_set_size'] }}"><br>
    <hr>

    <h2>Evaluator Frequency</h2>
    <p><i>
        Specify how often the evaluator should be trained. The units of this value is epochs. For example, specifying 40 would result in the evaluator being trained every
        40 epochs. Additionally, an evaluator will always be trained after the final epoch. The default for this value is {{ default_eval_freq }} epochs.
    </i></p>
    <b>Evaluator Frequency: </b><input name="image_eval_freq" id="image_eval_freq" type="number" min="0" step="1" value="{{ request.form['image_eval_freq'] }}"><br>
    <hr>

    <h2>Evaluator Parameters</h2>
    <p><i>
        Specify various training parameters for the evaluator here.
        The default value for these are: Number of evaluator epochs - {{ default_params.eval_num_epochs }}, Early stopping patience - {{ default_params.early_stopping_patience }}.
    </i></p>
    <b>Evaluator Epochs: </b><input name="eval_num_epochs" id="eval_num_epochs" type="number" min="1" step="1" value="{{ request.form['eval_num_epochs'] }}"><br>
    <b>Early Stopping Patience: </b><input name="early_stopping_patience" id="early_stopping_patience" type="number" min="1" step="1" value="{{ request.form['early_stopping_patience'] }}"><br>
    <br>

    <input type="submit" name="next" value="Next">
</form>
<form method=post><input type="submit" name="back" value="Back"></form>
<form method=post><input type="submit" name="cancel" value="Cancel"></form>
{% endblock %}